# ğŸš€ Airflow Retail Data Pipeline

A modern, production-ready data pipeline for retail analytics using **Apache Airflow**, **dbt**, **Soda**, and **BigQuery**. Designed for Astronomer or any Airflow-compatible environment.

---

## ğŸ—ºï¸ Architecture Overview

```
[CSV File] â†’ [GCS Bucket] â†’ [BigQuery: raw_table] â†’ [Soda check_load]
    â†’ [dbt transform] â†’ [Soda check_transform]
    â†’ [dbt report] â†’ [Soda check_report]
    â†’ [BigQuery: Target Tables] â†’ [BI Dashboard]
```

---

## ğŸ“¦ Dataset
- **Source:** [Kaggle Online Retail Dataset](https://www.kaggle.com/datasets/tunguz/online-retail)

## ğŸ¯ Project Purpose
A hands-on demonstration of a robust data engineering workflow for retail analytics:
- Automated ingestion of raw retail data from CSV to Google Cloud Storage (GCS)
- Loading and staging data in BigQuery
- Data transformation and modeling using dbt (dimensions, facts, reports)
- Data quality validation at each stage using Soda
- Orchestration and automation with Apache Airflow

## ğŸ› ï¸ Key Technologies
- **Apache Airflow**: Workflow orchestration and automation
- **Google Cloud Storage (GCS)**: Raw data storage
- **BigQuery**: Cloud data warehouse for scalable analytics
- **dbt**: SQL-based data transformation and modeling
- **Soda**: Data quality and validation

## ğŸ’¡ Typical Use Cases
- Retail sales analytics
- Customer segmentation and behavior analysis
- Product performance and inventory reporting
- Data quality monitoring in production pipelines

---

## ğŸ“ Project Structure

```text
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ retails.py                # Main Airflow DAG for the retail pipeline
â”œâ”€â”€ include/
â”‚   â”œâ”€â”€ dataset/                  # Source data files
â”‚   â”œâ”€â”€ dbt/
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ transform/        # dbt transformation models
â”‚   â”‚   â”‚   â””â”€â”€ report/           # dbt reporting models
â”‚   â”‚   â””â”€â”€ sources/              # dbt sources definition
â”‚   â”œâ”€â”€ gcp/
â”‚   â”‚   â””â”€â”€ service.json          # GCP service account (not in version control)
â”‚   â””â”€â”€ soda/
â”‚       â”œâ”€â”€ configuration.yml     # Soda configuration (uses env vars)
â”‚       â””â”€â”€ checks/               # Soda data quality checks
â”œâ”€â”€ airflow_settings.yaml         # Local Airflow settings (connections, variables)
â”œâ”€â”€ .env                          # environment variables
â”œâ”€â”€ .gitignore                    # Files/folders to ignore in git
â””â”€â”€ README.md                     # Project documentation (this file)
```

---

## âœ¨ Features
- **Airflow DAG** for orchestrating the retail data pipeline
- **dbt** for data transformation and reporting
- **Soda** for automated data quality checks
- **BigQuery** as the data warehouse
- **Environment variable support** for secrets and configuration
- **Modular and extensible**: Easily add new data sources, models, or checks

---

## âš¡ Quickstart
1. **Clone** this repository.
2. **Copy** `.env.example` to `.env` and fill in your credentials and secrets.
3. **Install** Python and OS dependencies:
   - `pip install -r requirements.txt`
   - Install any OS packages listed in `packages.txt`
4. **Start Airflow** using Astronomer CLI or your preferred method.
5. **Trigger** the `retail` DAG from the Airflow UI.

## ğŸ§ª Data Quality
Soda checks are defined in `include/soda/checks/` and configured via `include/soda/configuration.yml`.

## ğŸ—ï¸ Data Modeling
- dbt models are in `include/dbt/models/transform/` and `include/dbt/models/report/`
- Sources are defined in `include/dbt/sources/sources.yaml`
- Example models: `dim_customer.sql`, `dim_datetime.sql`, `dim_product.sql`, `fct_invoices.sql`
